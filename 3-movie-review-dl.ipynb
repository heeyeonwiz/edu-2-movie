{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. 영화 리뷰 데이터 딥러닝 트레이닝과 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1) 데이터 로드 : dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "\n",
    "PROCESSED_DATA_FILE = \"train_padded.p\"\n",
    "PROCESSED_LABEL_FILE = \"train_label.p\"\n",
    "\n",
    "PROCESSED_DATA_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_DATA_FILE)\n",
    "PROCESSED_LABEL_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DATA_PATH, \"rb\" ) as file:\n",
    "    train_padded = pickle.load(file)\n",
    "with open(PROCESSED_LABEL_PATH, \"rb\" ) as file:\n",
    "    train_y = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data 확인\n",
    "train_padded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file로 저장된 tokenizer를 읽어서 num_workds 값을 얻는다. 이 값은 vocab_size로 사용한다.\n",
    "\n",
    "TOKENIZED_FILE = 'tokenized.json'\n",
    "TOKENIZED_PATH = os.path.join(DATA_DIR, TOKENIZED_FILE)\n",
    "\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "with open(TOKENIZED_PATH) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_config()['num_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2) 모델 만들기와 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (가) 네트워크 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'model'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SUMMARY_FILE = \"movie_review_model.png\"\n",
    "MODEL_SUMMARY_PATH = os.path.join(MODEL_DIR, MODEL_SUMMARY_FILE)\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# tf.keras.utils.plot_model(model, to_file = model_dir + 'cifar10_cnn_model.png', show_shapes=True)\n",
    "plot_model(model, to_file = MODEL_SUMMARY_PATH, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "MODEL_FILE = \"best_model.h5\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n",
    "\n",
    "mc = ModelCheckpoint(MODEL_PATH, monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=15, callbacks=[es, mc], batch_size=300, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 측정값의 시각화 입니다.  \n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,label='Training Loss')\n",
    "plt.plot(epochs,val_loss,label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 측정값의 시각화 입니다.  \n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1,len(loss)+1)\n",
    "plt.plot(epochs,acc,label='Training Accuarcy')\n",
    "plt.plot(epochs,val_acc,label='Validation Accuarcy')\n",
    "plt.title('Training and Validation Accuarcy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 10) 테스트 ; 추론(Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (가) 데스트 데이터로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_TEST_DATA_FILE = \"test_padded.p\"\n",
    "PROCESSED_TEST_LABEL_FILE = \"test_label.p\"\n",
    "\n",
    "PROCESSED_TEST_DATA_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_TEST_DATA_FILE)\n",
    "PROCESSED_TEST_LABEL_PATH = os.path.join(PROCESSED_DATA_DIR, PROCESSED_TEST_LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_TEST_DATA_PATH, \"rb\" ) as file:\n",
    "    test_padded = pickle.load(file)\n",
    "with open(PROCESSED_TEST_LABEL_PATH, \"rb\" ) as file:\n",
    "    test_y = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_padded\n",
    "test_Y = test_y\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(test_X, test_Y, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(test_X[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (나) 문장을 입력하여 추론하는 프로그램\n",
    "  1. 형태소 분석\n",
    "  2. 시퀀스 만들기\n",
    "  3. 패딩\n",
    "  4. 추론 : model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morphed_word(wiki_result):\n",
    "    # print('wiki_result : ', wiki_result[0][0])\n",
    "    txt = []\n",
    "    for i in wiki_result[0][0]:\n",
    "        txt.append(i[0])\n",
    "    # print('wiki_result, txt : ', txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "# 형태소 분석 함수\n",
    "def get_morph(input_text):\n",
    "    # print('get_morph : ', input_text)\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "    morphed = get_morphed_word(kiwi.analyze(input_text))\n",
    "    # print('morphed : ', morphed)\n",
    "    \n",
    "    return morphed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = 30 # 시퀀스의 길이를 30으로 고정\n",
    "trunc_type = \"post\" # 길이가 30 보다 길 때 뒷 부분을 버린다. \n",
    "padding_type = \"post\" # 길이가 30 보다 짧을 대 뒷 부분을 0으로 채운다.\n",
    "\n",
    "# padding : 뒤를 0으로 채운다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def sentiment_predict(sentence):\n",
    "    morphed = get_morph(sentence)\n",
    "    # print(morphed)\n",
    "    encoded = tokenizer.texts_to_sequences([morphed]) # 정수 인코딩\n",
    "    # print('encoded : ', encoded)\n",
    "    padded = pad_sequences(encoded, maxlen = max_len, padding=padding_type, truncating=trunc_type) # 패딩\n",
    "    # print(padded)\n",
    "    score = float(model.predict(padded)) # 예측\n",
    "    \n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = '이 영화 정말 재밌네!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 핵노잼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('뭐 이런 영화가 다있어?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('감독 뭐하는 놈이냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('재미있네요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
